{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import dataiku\n",
    "import pandas as pd, numpy as np\n",
    "from dataiku import pandasutils as pdu\n",
    "import concurrent.futures\n",
    "\n",
    "# Read recipe inputs\n",
    "protein_compatibility_particles = dataiku.Folder(\"PIojoFlG\")\n",
    "protein_compatibility_particles_info = protein_compatibility_particles.get_info()\n",
    "particles_from_PS20_degradation = dataiku.Folder(\"59PiMVgQ\")\n",
    "particles_from_PS20_degradation_info = particles_from_PS20_degradation.get_info()\n",
    "contact_with_bag_surface = dataiku.Folder(\"AZvdljVp\")\n",
    "contact_with_bag_surface_info = contact_with_bag_surface.get_info()\n",
    "protein_stir = dataiku.Folder(\"Y8jS78ta\")\n",
    "protein_stir_info = protein_stir.get_info()\n",
    "protein_vortex = dataiku.Folder(\"A0nzUMQ2\")\n",
    "protein_vortex_info = protein_vortex.get_info()\n",
    "siliconOil_droplets_from_PFS = dataiku.Folder(\"Oay7P2WG\")\n",
    "siliconOil_droplets_from_PFS_info = siliconOil_droplets_from_PFS.get_info()\n",
    "silicon_Catheters = dataiku.Folder(\"mbeOHgg1\")\n",
    "silicon_Catheters_info = silicon_Catheters.get_info()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Write recipe outputs\n",
    "svp_clusters = dataiku.Folder(\"hAbB1mJR\")\n",
    "svp_clusters_info = svp_clusters.get_info()\n",
    "svp_clusters_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import all the necessary packages\n",
    "import dataiku\n",
    "import pandas as pd, numpy as np\n",
    "from dataiku import pandasutils as pdu\n",
    "import cv2\n",
    "import io\n",
    "import time\n",
    "from keras.models import load_model, Model\n",
    "from keras.preprocessing import image\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from keras.preprocessing import image\n",
    "import matplotlib.image as mpimg\n",
    "from PIL import Image, ImageStat\n",
    "from functools import reduce\n",
    "import os, shutil, glob, os.path\n",
    "from PIL import Image as pil_image\n",
    "image.LOAD_TRUNCATED_IMAGES = True\n",
    "model = VGG16(weights='imagenet', include_top=False)\n",
    "\n",
    "map_folder_labels = {    'Y8jS78ta': 'p4',\n",
    "                         'A0nzUMQ2': 'p5',\n",
    "                         'Oay7P2WG': 's1',\n",
    "                         'mbeOHgg1': 's2',\n",
    "                         'PIojoFlG': 'p1',\n",
    "                         '59PiMVgQ': 'p3',\n",
    "                         'AZvdljVp': 'p2'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read csv filE\n",
    "def get_image_data(name_folder, label_name):\n",
    "    conn_folder = dataiku.Folder(name_folder)\n",
    "    all_files_paths = conn_folder.list_paths_in_partition()\n",
    "    csv_paths = [file_path for file_path in all_files_paths if file_path.endswith('.csv')]\n",
    "    complete_df = pd.DataFrame()\n",
    "    for path in csv_paths:\n",
    "        lis_records = []\n",
    "        with conn_folder.get_download_stream(path) as f:\n",
    "            byte_data = f.readlines()\n",
    "            for row in byte_data:\n",
    "                #print(row)\n",
    "                lis_records.append(row.decode('utf-8', 'ignore').replace('\\r\\n','').split(',')[1:])\n",
    "        filename = lis_records[10][0]\n",
    "        #print(filename)\n",
    "        folder_path = filename.split(\"\\\\\")[-2]\n",
    "        #print(folder_path)\n",
    "        df = pd.DataFrame(lis_records[29:], columns = lis_records[28])\n",
    "        df['Folder_name']= name_folder\n",
    "        df[\"Folder_path\"] = folder_path\n",
    "        df[\"Label_name\"] = label_name\n",
    "        df[\"Image_path\"] =  df['Folder_path']+ \"/ImageFolder/Image_\" + df[' Frame #'].str.zfill(5) + '.jpg'\n",
    "        if complete_df.empty:\n",
    "            complete_df = df\n",
    "        else:\n",
    "            complete_df = pd.concat([df, complete_df])\n",
    "    return complete_df\n",
    "# a=get_image_data('sipENqLc', 'p1')\n",
    "# print(a.head().values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Image read/write and export the directories to csv file\n",
    "start_time = time.time()\n",
    "def get_all_folders_data():\n",
    "    all_folder_data = pd.DataFrame()\n",
    "    for key, value in map_folder_labels.items():\n",
    "        folder_data = get_image_data(key, value)\n",
    "        if all_folder_data.empty:\n",
    "            all_folder_data = folder_data\n",
    "        else:\n",
    "            all_folder_data = pd.concat([folder_data, all_folder_data])\n",
    "    all_folder_data.to_csv('/hadoopfs/fs1/dataiku/data_dir/managed_folders/PARTICULATEIMAGECLASSIFICATIONONSVP/hAbB1mJR/complete_csv_data.csv', index = False)\n",
    "get_all_folders_data()\n",
    "end_time = time.time()\n",
    "elsp_time = end_time-start_time\n",
    "print(elsp_time/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#multi processing\n",
    "import multiprocessing\n",
    "def folders_connection():\n",
    "    p1 = dataiku.Folder(\"PIojoFlG\")\n",
    "    p3 = dataiku.Folder(\"59PiMVgQ\")\n",
    "    p2 = dataiku.Folder(\"AZvdljVp\")\n",
    "    p4 = dataiku.Folder(\"Y8jS78ta\")\n",
    "    p5 = dataiku.Folder(\"A0nzUMQ2\")\n",
    "    s1 = dataiku.Folder(\"Oay7P2WG\")\n",
    "    s2 = dataiku.Folder(\"mbeOHgg1\")\n",
    "    return p1,p2,p3,p4,p5,s1,s2\n",
    "p1,p2,p3,p4,p5,s1,s2 = folders_connection()\n",
    "def check_folder_obj(folder_name):\n",
    "    if folder_name == 'p1':\n",
    "        return p1\n",
    "    elif folder_name == 'p2':\n",
    "        return p2\n",
    "    elif folder_name == 'p3':\n",
    "        return p3\n",
    "    elif folder_name == 'p4':\n",
    "        return p4\n",
    "    elif folder_name == 'p5':\n",
    "        return p5\n",
    "    elif folder_name == 's1':\n",
    "        return s1\n",
    "    elif folder_name == 's2':\n",
    "        return s2\n",
    "\n",
    "import time\n",
    "st = time.time()\n",
    "base_dir = '/hadoopfs/fs1/dataiku/data_dir/managed_folders/PARTICULATEIMAGECLASSIFICATIONONSVP/hAbB1mJR/Cropped_images/'\n",
    "df = pd.read_csv('/hadoopfs/fs1/dataiku/data_dir/managed_folders/PARTICULATEIMAGECLASSIFICATIONONSVP/hAbB1mJR/complete_csv_data.csv', chunksize = 10000)\n",
    "for i, reader in enumerate(df):\n",
    "    processes = []\n",
    "    reader['unq'] = reader.groupby('Image_path').cumcount().astype(str)\n",
    "    for idx, row in reader.iterrows():\n",
    "        p = multiprocessing.Process(target = crop_save_image, args = (row,))\n",
    "        processes.append(p)\n",
    "        p.start()\n",
    "    for process in processes:\n",
    "        process.join()\n",
    "    break\n",
    "et = time.time()\n",
    "print((et-st)/3600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_save_image(row):\n",
    "    image_path = row['Image_path']\n",
    "    folder_obj = check_folder_obj(row['Label_name'])\n",
    "    pil_image=Image.open(folder_obj.get_download_stream(image_path)) # open image using PIL\n",
    "    x_left,y_top,x_right,y_bottom = int(row1['X Left']),int(row1['Y Top']),int(row1['X Right']), int(row1['Y Bottom'])\n",
    "    if x_left == 0 and x_right == 0:\n",
    "        x_left,x_right = 1, 2\n",
    "    if y_top == 0 and y_bottom == 0:\n",
    "        y_top,y_bottom = 1,2\n",
    "    cropped_img = pil_image.crop((x_left,y_top,x_right,y_bottom))\n",
    "    if len(unq) == 0:\n",
    "        output_path = base_dir + str(row['Label_name']) + \"_\" + row['Folder_path']+\"_Image_\"+ str(row[' Frame #'])+'.jpg'\n",
    "    else:\n",
    "        output_path = base_dir + str(row['Label_name']) + \"_\" + row['Folder_path']+\"_Image_\"+ str(row[' Frame #'])+ \"_\"+str(row['unq'])+'.jpg'\n",
    "    cropped_img.save(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_folder_data = pd.read_csv('/hadoopfs/fs1/dataiku/data_dir/managed_folders/PARTICULATEIMAGECLASSIFICATIONONSVP/hAbB1mJR/complete_csv_data.csv')\n",
    "base_dir = '/hadoopfs/fs1/dataiku/data_dir/managed_folders/PARTICULATEIMAGECLASSIFICATIONONSVP/hAbB1mJR/Cropped_images/'\n",
    "folder_names = list(map_folder_labels.keys())\n",
    "for f_name in folder_names:\n",
    "    st = time.time()\n",
    "    folder_obj = dataiku.Folder(f_name)\n",
    "    print(\"Started folder:\", f_name)\n",
    "    folder_data = all_folder_data[all_folder_data['Folder_name']==f_name]\n",
    "    try:\n",
    "        unq_data = folder_data.drop_duplicates(subset = ['Folder_path', ' Frame #'])\n",
    "        for idx, row in unq_data.iterrows():\n",
    "            image_data = folder_data[(folder_data['Folder_path'] == row['Folder_path']) & (folder_data[' Frame #'] == row[' Frame #'])]\n",
    "            image_data.reset_index(inplace=True)\n",
    "            image_data.sort_values(by = ['Folder_path',' Frame #'], axis = 0, inplace = True)\n",
    "            for idx1, row1 in image_data.iterrows():\n",
    "                image_path = row['Image_path']\n",
    "                pil_image=Image.open(folder_obj.get_download_stream(image_path)) # open image using PIL\n",
    "                x_left,y_top,x_right,y_bottom = int(row1['X Left']),int(row1['Y Top']),int(row1['X Right']), int(row1['Y Bottom'])\n",
    "                if x_left == 0 and x_right == 0:\n",
    "                    x_left,x_right = 1, 2\n",
    "                if y_top == 0 and y_bottom == 0:\n",
    "                    y_top,y_bottom = 1,2\n",
    "                cropped_img = pil_image.crop((x_left,y_top,x_right,y_bottom))\n",
    "                if len(image_data) == 1:\n",
    "                    output_path = base_dir + str(row['Label_name']) + \"_\" + row['Folder_path']+\"_Image_\"+ str(row[' Frame #'])+'.jpg'\n",
    "                else:\n",
    "                    output_path = base_dir + str(row['Label_name']) + \"_\" + row['Folder_path']+\"_Image_\"+ str(row[' Frame #'])+ \"_\"+str(idx1)+'.jpg'\n",
    "                try:\n",
    "                    cropped_img.save(output_path)\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    print(row1)\n",
    "                    print(output_path)\n",
    "                    print(x_left,x_right,y_top,y_bottom)\n",
    "    except Exception as e1:\n",
    "        print(e1)\n",
    "        print(image_path)\n",
    "\n",
    "    et = time.time()\n",
    "    esp = et - st\n",
    "    print(\"Time Elaspsed for\", f_name, esp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_folder_data = pd.read_csv('/hadoopfs/fs1/dataiku/data_dir/managed_folders/PARTICULATEIMAGECLASSIFICATIONONSVP/hAbB1mJR/complete_csv_data.csv')\n",
    "base_dir = '/hadoopfs/fs1/dataiku/data_dir/managed_folders/PARTICULATEIMAGECLASSIFICATIONONSVP/hAbB1mJR/Filtered_cropped_Images/'\n",
    "folder_names = list(map_folder_labels.keys())\n",
    "for f_name in folder_names:\n",
    "    st = time.time()\n",
    "    folder_obj = dataiku.Folder(f_name)\n",
    "    print(\"Started folder:\", f_name)\n",
    "    folder_data = all_folder_data[all_folder_data['Folder_name']==f_name]\n",
    "    try:\n",
    "        unq_data = folder_data.drop_duplicates(subset = ['Folder_path', ' Frame #'])\n",
    "        for idx, row in unq_data.iterrows():\n",
    "            image_data = folder_data[(folder_data['Folder_path'] == row['Folder_path']) & (folder_data[' Frame #'] == row[' Frame #'])]\n",
    "            image_data.reset_index(inplace=True)\n",
    "            image_data.sort_values(by = ['Folder_path',' Frame #'], axis = 0, inplace = True)\n",
    "            for idx1, row1 in image_data.iterrows():\n",
    "                image_path = row['Image_path']\n",
    "                pil_image=Image.open(folder_obj.get_download_stream(image_path)) # open image using PIL\n",
    "                x_left,y_top,x_right,y_bottom = int(row1['X Left']),int(row1['Y Top']),int(row1['X Right']), int(row1['Y Bottom'])\n",
    "                # Creating cropping width for edge-sided particulates\n",
    "                if (x_right - x_left) < 1:\n",
    "                    x_left,x_right = 1, 2\n",
    "                if (y_bottom - y_top) <1:\n",
    "                    y_top,y_bottom = 1,2\n",
    "                total_eles = (x_right - x_left) * (y_bottom - y_top)\n",
    "                # Maintaining considerbale crop size\n",
    "                if total_eles > 100:\n",
    "                    cropped_img = pil_image.crop((x_left,y_top,x_right,y_bottom))\n",
    "                    array_data = np.asarray(cropped_img)\n",
    "                    # Checking the black spot intensity\n",
    "                    black_pixel_count = np.count_nonzero(array_data < 100)\n",
    "                    # Save the image if contains reasonable amount of blackspots\n",
    "                    if black_pixel_count:\n",
    "                        if len(image_data) == 1:\n",
    "                            output_path = base_dir + str(row['Label_name']) + \"_\" + row['Folder_path']+\"_Image_\"+ str(row[' Frame #'])+'.jpg'\n",
    "                        else:\n",
    "                            output_path = base_dir + str(row['Label_name']) + \"_\" + row['Folder_path']+\"_Image_\"+ str(row[' Frame #'])+ \"_\"+str(idx1)+'.jpg'\n",
    "                        try:\n",
    "                            cropped_img.save(output_path)\n",
    "                        except Exception as e:\n",
    "                            print(e)\n",
    "                            print(row1)\n",
    "                            print(output_path)\n",
    "                            print(x_left,x_right,y_top,y_bottom)\n",
    "    except Exception as e1:\n",
    "        print(e1)\n",
    "        print(image_path)\n",
    "\n",
    "    et = time.time()\n",
    "    esp = et - st\n",
    "    print(\"Time Elaspsed for\", f_name, esp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "(6137+1186+196+21+130+76+4474)/3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #test image cropping\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# with (protein_compatibility_particles.get_download_stream('30 SS093 F3 Delivered R1-Run009_5200-3244_210121-230015/ImageFolder/Image_01657.jpg')) as f:\n",
    "#     im = Image.open( f).convert('L')\n",
    "#     cropped_img = im.crop((398,0,409,0))\n",
    "#     print(cropped_img)\n",
    "#     plt.imshow(cropped_img)\n",
    "#     plt.show()\n",
    "\n",
    "#     #img2.save(\"/hadoopfs/fs1/dataiku/data_dir/managed_folders/IMAGECLASSIFICATION/JUMGKyul/test_image.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering using pretrained VGG16 -Transfer Learning\n",
    "\n",
    "reverse_map_folder_labels = {'p1' : 'PIojoFlG',\n",
    "                     'p3' : '59PiMVgQ',\n",
    "                     'p2' : 'AZvdljVp',\n",
    "                     'p4' : 'Y8jS78ta',\n",
    "                     'p5' : 'A0nzUMQ2',\n",
    "                     's1' : 'Oay7P2WG',\n",
    "                     's2' : 'mbeOHgg1'}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Variables\n",
    "# imdir = 'C:/indir/'\n",
    "targetdir = \"/hadoopfs/fs1/dataiku/data_dir/managed_folders/PARTICULATEIMAGECLASSIFICATIONONSVP/hAbB1mJR\"\n",
    "number_clusters = 7\n",
    "df = pd.read_csv(\"/hadoopfs/fs1/dataiku/data_dir/managed_folders/PARTICULATEIMAGECLASSIFICATIONONSVP/hAbB1mJR/all_images.csv\")\n",
    "# Loop over files and get features\n",
    "start_time = time.time()\n",
    "featurelist = []\n",
    "filelist = list(df.image_paths)\n",
    "for index, row in df.iterrows():\n",
    "    folder_obj = dataiku.Folder(reverse_map_folder_labels[row['label']])\n",
    "    img=Image.open(folder_obj.get_download_stream(row['image_paths']))#.convert('L') # open image using PIL\n",
    "    img_data = np.array(img)\n",
    "    #img_data = image.img_to_array(img)\n",
    "    img_data = np.expand_dims(img_data, axis=0)\n",
    "    img_data = preprocess_input(img_data)\n",
    "    features = np.array(img_data)\n",
    "    #features = np.array(model.predict(img_data))\n",
    "    featurelist.append(features.flatten())\n",
    "\n",
    "# Clustering\n",
    "kmeans = KMeans(n_clusters=number_clusters, random_state=0).fit(np.array(featurelist))\n",
    "\n",
    "# Copy images renamed by cluster\n",
    "# Check if target dir exists\n",
    "# Copy with cluster na\n",
    "cluster = [m for i,m in enumerate(kmeans.labels_)]\n",
    "df[\"Cluster_name\"] = cluster\n",
    "\n",
    "df.to_csv(\"/hadoopfs/fs1/dataiku/data_dir/managed_folders/PARTICULATEIMAGECLASSIFICATIONONSVP/hAbB1mJR/images_cluster.csv\")\n",
    "# for i, m in enumerate(kmeans.labels_):\n",
    "#     print(\"    Copy: %s / %s\" %(i, len(kmeans.labels_)), end=\"\\r\")\n",
    "#     print(filelist[i], targetdir + str(m) + \"_\" + str(i) + \".jpg\", str(m))\n",
    "end_time = time.time()\n",
    "elapsed_time= end_time-start_time\n",
    "print('Time taken =', elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering\n",
    "kmeans = KMeans(n_clusters=number_clusters, random_state=0).fit(np.array(featurelist))\n",
    "\n",
    "# Copy images renamed by cluster\n",
    "# Check if target dir exists\n",
    "# Copy with cluster na\n",
    "cluster = [m for i,m in enumerate(kmeans.labels_)]\n",
    "df[\"Cluster_name\"] = cluster\n",
    "\n",
    "df.to_csv(\"/hadoopfs/fs1/dataiku/data_dir/managed_folders/IMAGECLASSIFICATION/JUMGKyul/images_cluster.csv\")\n",
    "# for i, m in enumerate(kmeans.labels_):\n",
    "#     print(\"    Copy: %s / %s\" %(i, len(kmeans.labels_)), end=\"\\r\")\n",
    "#     print(filelist[i], targetdir + str(m) + \"_\" + str(i) + \".jpg\", str(m))\n",
    "end_time = time.time()\n",
    "elapsed_time= end_time-start_time\n",
    "print('Time taken =', elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = [m for i,m in enumerate(kmeans.labels_)]\n",
    "print(len(cluster))\n",
    "df1 = df.head(21)\n",
    "df1[\"Cluster_name\"] = cluster\n",
    "\n",
    "df1.to_csv(\"/hadoopfs/fs1/dataiku/data_dir/managed_folders/IMAGECLASSIFICATION/JUMGKyul/top_20_images_cluster.csv\")"
   ]
  }
 ],
 "metadata": {
  "associatedRecipe": "compute_hAbB1mJR",
  "creator": "kaushik.pradeep",
  "customFields": {},
  "kernelspec": {
   "display_name": "Python in ExternalCompute-GPU (env Python36-GPU)",
   "language": "python",
   "name": "py-dku-containerized-venv-python36-gpu-externalcompute-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "tags": [
   "recipe-editor"
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
