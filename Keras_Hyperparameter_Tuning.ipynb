{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import dataiku\n",
    "import pandas as pd, numpy as np\n",
    "from dataiku import pandasutils as pdu\n",
    "\n",
    "# Read recipe inputs\n",
    "svp_clusters = dataiku.Folder(\"hAbB1mJR\")\n",
    "svp_clusters_info = svp_clusters.get_info()\n",
    "\n",
    "\n",
    "# Compute recipe outputs\n",
    "# TODO: Write here your actual code that computes the outputs\n",
    "# NB: DSS supports several kinds of APIs for reading and writing data. Please see doc.\n",
    "\n",
    "#hyperparameter_Tuning_df = ... # Compute a Pandas dataframe to write into Hyperparameter_Tuning\n",
    "\n",
    "\n",
    "# # Write recipe outputs\n",
    "# hyperparameter_Tuning = dataiku.Dataset(\"Hyperparameter_Tuning\")\n",
    "# hyperparameter_Tuning.write_with_schema(hyperparameter_Tuning_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import dataiku\n",
    "import pandas as pd, numpy as np\n",
    "from dataiku import pandasutils as pdu\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D , MaxPool2D , Flatten , Dropout\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib as plt\n",
    "# Read recipe inputs\n",
    "svp_clusters = dataiku.Folder(\"hAbB1mJR\")\n",
    "svp_clusters_info = svp_clusters.get_info()\n",
    "\n",
    "\n",
    "# Compute recipe outputs\n",
    "# TODO: Write here your actual code that computes the outputs\n",
    "# NB: DSS supports several kinds of APIs for reading and writing data. Please see doc.\n",
    "\n",
    "# cnn_result_df = ... # Compute a Pandas dataframe to write into CNN_result\n",
    "\n",
    "\n",
    "# # Write recipe outputs\n",
    "# cnn_result = dataiku.Dataset(\"CNN_result\")\n",
    "# cnn_result.write_with_schema(cnn_result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "conn_folder = dataiku.Folder('hAbB1mJR')\n",
    "all_files_paths = conn_folder.list_paths_in_partition()\n",
    "image_paths = [file_path for file_path in all_files_paths if file_path.endswith(\".jpg\")]\n",
    "label_names = []\n",
    "resized_images = []\n",
    "for idx ,image_path in enumerate(image_paths):\n",
    "    label_name = os.path.basename(image_path)[:2]\n",
    "    pil_image=Image.open(conn_folder.get_download_stream(image_path))\n",
    "    #resized_img = pil_image.resize((20,20))\n",
    "    resized_images.append(pil_image)\n",
    "    label_names.append(label_name)\n",
    "    if idx == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(20,10))\n",
    "columns = 5\n",
    "for i, image in enumerate(resized_images):\n",
    "    plt.subplot(len(resized_images) / columns + 1, columns, i + 1)\n",
    "    plt.imshow(image, cmap = 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipyplot\n",
    "\n",
    "ipyplot.plot_images(resized_images, max_images=20, img_width=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import asarray\n",
    "labels = ['protein', 'silicon']\n",
    "img_size = 20\n",
    "\n",
    "def get_data(image_paths):\n",
    "    data = []\n",
    "    labels = []\n",
    "    sil_cnt = 0\n",
    "    prot_cnt = 0\n",
    "    for image_path in image_paths:\n",
    "            try:\n",
    "                label_name = os.path.basename(image_path)[:2]\n",
    "                if label_name == 'p5':\n",
    "                    class_num = 0\n",
    "                    prot_cnt+=1\n",
    "                    if prot_cnt > 1000:\n",
    "                        continue\n",
    "                else:\n",
    "                    class_num = 1\n",
    "                    sil_cnt+=1\n",
    "                    if sil_cnt >1000:\n",
    "                        continue\n",
    "                if prot_cnt > 1000 and sil_cnt >1000:\n",
    "                    break\n",
    "                pil_image=Image.open(conn_folder.get_download_stream(image_path))\n",
    "                resized_img = pil_image.resize((20,20))\n",
    "                resized_img = np.expand_dims(resized_img, axis=-1)\n",
    "                resized_arr = asarray(resized_img)\n",
    "                data.append(resized_arr)\n",
    "                labels.append(class_num)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "\n",
    "    return np.array(data),labels\n",
    "X,y = get_data(image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "l = []\n",
    "for i in y:\n",
    "    if(i == 0):\n",
    "        l.append(\"Protein\")\n",
    "    else:\n",
    "        l.append(\"Silicon\")\n",
    "sns.set_style('darkgrid')\n",
    "sns.countplot(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start=time.time()\n",
    "\n",
    "# define a function to create model, required for KerasClassifier\n",
    "# the function takes drop_out rate as argument so we can optimize it\n",
    "def create_cnn_model(dropout_rate=0):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32,3,padding=\"same\", activation=\"relu\", input_shape=(20,20,1)))\n",
    "    model.add(MaxPool2D())\n",
    "    model.add(Conv2D(32, 3, padding=\"same\", activation='relu'))\n",
    "    model.add(MaxPool2D())\n",
    "    model.add(Conv2D(64, 3, padding=\"same\", activation='relu'))\n",
    "    model.add(MaxPool2D())\n",
    "    model.add(Dropout(rate = dropout_rate))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128,activation=\"sigmoid\"))\n",
    "    model.add(Dense(2, activation=\"softmax\"))\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy'],\n",
    "        )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_cv_results(search_results):\n",
    "    print('Best score = {:.4f} using {}'.format(search_results.best_score_, search_results.best_params_))\n",
    "    means = search_results.cv_results_['mean_test_score']\n",
    "    stds = search_results.cv_results_['std_test_score']\n",
    "    params = search_results.cv_results_['params']\n",
    "    for mean, stdev, param in zip(means, stds, params):\n",
    "        print('mean test accuracy +/- std = {:.4f} +/- {:.4f} with: {}'.format(mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "from keras.utils import to_categorical\n",
    "model = KerasClassifier(build_fn=create_cnn_model, verbose=1)\n",
    "# define parameters and values for grid search\n",
    "param_grid = {\n",
    "    'batch_size': [64, 128],\n",
    "    'epochs': [50, 100,200],\n",
    "    'dropout_rate': [0.10, 0.20],\n",
    "    #'conv_activation' : ['sigmoid', 'tanh', 'relu']\n",
    "}\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "grid_result = grid.fit(X, to_categorical(y))  # fit the full dataset as we are using cross validation\n",
    "\n",
    "# print out results\n",
    "print('time for grid search = {:.0f} sec'.format(time.time()-start))\n",
    "display_cv_results(grid_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32,3,padding=\"same\", activation=\"relu\", input_shape=(20,20,1)))\n",
    "model.add(MaxPool2D())\n",
    "\n",
    "model.add(Conv2D(32, 3, padding=\"same\", activation=\"relu\"))\n",
    "model.add(MaxPool2D())\n",
    "\n",
    "model.add(Conv2D(64, 3, padding=\"same\", activation=\"relu\"))\n",
    "model.add(MaxPool2D())\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128,activation=\"relu\"))\n",
    "model.add(Dense(2, activation=\"softmax\"))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "opt = Adam(lr=0.000001)\n",
    "model.compile(optimizer = opt , loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) , metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train,y_train, batch_size = 128, epochs = 200 , validation_data = (X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(200)\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Test Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Test Accuracy')\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Test Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Test Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict_classes(X_test)\n",
    "predictions = predictions.reshape(1,-1)[0]\n",
    "print(classification_report(y_test, predictions, target_names = ['Protein','Silicon']))"
   ]
  }
 ],
 "metadata": {
  "associatedRecipe": "compute_Hyperparameter_Tuning",
  "creator": "satya.yendamuri",
  "customFields": {},
  "kernelspec": {
   "display_name": "Python (env Python36-GPU)",
   "language": "python",
   "name": "py-dku-venv-python36-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "tags": [
   "recipe-editor"
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
